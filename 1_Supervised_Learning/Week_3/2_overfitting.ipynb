{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what is overfitting? To help us understand what is overfitting. Let's take a look at a few examples. Let's go back to our original example of predicting housing prices with linear regression. Where you want to predict the price as a function of the size of a house. To help us understand what is overfitting, let's take a look at a linear regression example. I'm going to go back to our original running example of predicting housing prices with linear regression. \n",
    "\n",
    "**Graph on Left (Underfitting):** Suppose your data-set looks like this, with the input feature x being the size of the house, and the value, y that you're trying to predict the price of the house. One thing you could do is fit a linear function to this data. If you do that, you get a straight line fit to the data that maybe looks like this. But this isn't a very good model. Looking at the data, it seems pretty clear that as the size of the house increases, the housing process flattened out. \n",
    "\n",
    "This algorithm does not fit the training data very well. The technical term for this is the model is underfitting the training data. Another term is the algorithm has high bias. \n",
    "\n",
    "Another way to think of this form of bias is as if the learning algorithm has a very strong preconception, or we say a very strong bias, that the housing prices are going to be a completely linear function of the size despite data to the contrary. This preconception that the data is linear causes it to fit a straight line that fits the data poorly, leading it to underfitted data. \n",
    "\n",
    "**Graph in Middle (Just Right):** Now, let's look at a second variation of a model, which is if you insert for a quadratic function at the data with two features, x and x^2, then when you fit the parameters W1 and W2, you can get a curve that fits the data somewhat better. Maybe it looks like this. Also, if you were to get a new house, that's not in this set of five training examples. This model would probably do quite well on that new house. \n",
    "\n",
    "If you're real estate agents, the idea that you want your learning algorithm to do well, even on examples that are not on the training set, that's called **generalization**. Technically we say that you want your learning algorithm to generalize well, which means to make good predictions even on brand new examples that it has never seen before. These quadratic models seem to fit the training set not perfectly, but pretty well. I think it would generalize well to new examples. \n",
    "\n",
    "**Graph on Right (Overfitting):** Now let's look at the other extreme. What if you were to fit a fourth-order polynomial to the data? You have x, x^2, x^3, and x^4 all as features. With this fourth for the polynomial, you can actually fit the curve that passes through all five of the training examples exactly. You might get a curve that looks like this. This, on one hand, seems to do an extremely good job fitting the training data because it passes through all of the training data perfectly. In fact, you'd be able to choose parameters that will result in the cost function being exactly equal to zero because the errors are zero on all five training examples. But this is a very wiggly curve, its going up and down all over the place. If you have this whole size right here, the model would predict that this house is cheaper than houses that are smaller than it. We don't think that this is a particularly good model for predicting housing prices. \n",
    "\n",
    "The technical term is that we'll say this model has overfit the data, or this model has an overfitting problem. Because even though it fits the training set very well, it has fit the data almost too well, hence is overfit. It does not look like this model will generalize to new examples that's never seen before. Another term for this is that the algorithm has **high variance**. \n",
    "\n",
    "In machine learning, many people will use the terms over-fit and high-variance almost interchangeably. We'll use the terms underfit and high bias almost interchangeably. The intuition behind overfitting or high-variance is that the algorithm is trying very hard to fit every single training example. It turns out that if your training set were just even a little bit different, say one holes was priced just a little bit more little bit less, then the function that the algorithm fits could end up being totally different. \n",
    "\n",
    "If two different machine learning engineers were to fit this fourth-order polynomial model, to just slightly different datasets, they couldn't end up with totally different predictions or highly variable predictions. That's why we say the algorithm has high variance. Contrasting this rightmost model with the one in the middle for the same house, it seems, the middle model gives them much more reasonable prediction for price. There isn't really a name for this case in the middle, but I'm just going to call this **just right**, because it is neither underfit nor overfit. \n",
    "\n",
    "**ML Goal:** You can say that the goal machine learning is to find a model that hopefully is neither underfitting nor overfitting. In other words, hopefully, a model that has neither high bias nor high variance. When I think about underfitting and overfitting, high bias and high variance. \n",
    "\n",
    "<img src=\"images/of1.png\" style=\"width:700px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/of2.png\" style=\"width:700px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing overfitting\n",
    "\n",
    "<img src=\"images/aof1.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/aof2.png\" style=\"width:700px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "If you look at an overfit model, here's a model using polynomial features: x, x², x³, and so on. You find that the parameters are often relatively large. \n",
    "\n",
    "Now if you were to eliminate some of these features, say, if you were to eliminate the feature x4, that corresponds to setting this parameter to 0. So setting a parameter to 0 is equivalent to eliminating a feature, which is what we saw on the previous slide. \n",
    "\n",
    "**It turns out that regularization is a way to more gently reduce the impacts of some of the features without doing something as harsh as eliminating it outright. What regularization does is encourage the learning algorithm to shrink the values of the parameters without necessarily demanding that the parameter is set to exactly 0.** \n",
    "\n",
    "It turns out that even if you fit a higher order polynomial like this, so long as you can get the algorithm to use smaller parameter values: w1, w2, w3, w4. You end up with a curve that ends up fitting the training data much better. \n",
    "\n",
    "**So what regularization does, is it lets you keep all of your features, but they just prevents the features from having an overly large effect, which is what sometimes can cause overfitting.** \n",
    "\n",
    "By the way, by convention, we normally just reduce the size of the wj parameters, that is w1 through wn. It doesn't make a huge difference whether you regularize the parameter b as well, you could do so if you want or not if you don't. I usually don't and it's just fine to regularize w1, w2, all the way to wn, but not really encourage b to become smaller. In practice, it should make very little difference whether you also regularize b or not. \n",
    "\n",
    "<img src=\"images/aof3.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "\n",
    "<img src=\"images/aof4.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function with regularization\n",
    "\n",
    "<img src=\"images/cfr1.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "Let's jump in, recall this example from the previous video in which we saw that if you fit a quadratic function to this data, it gives a pretty good fit. But if you fit a very high order polynomial, you end up with a curve that over fits the data. But now consider the following, suppose that you had a way to make the parameters W3 and W4 really, really small. Say close to 0. Here's what I mean. \n",
    "\n",
    "Let's say instead of minimizing this objective function, this is a cost function for linear regression. Let's say you were to modify the cost function and add to it 1000 times W3 squared plus 1000 times W4 squared. And here I'm just choosing 1000 because it's a big number but any other really large number would be okay. So with this modified cost function, you could in fact be penalizing the model if W3 and W4 are large. Because if you want to minimize this function, the only way to make this new cost function small is if W3 and W4 are both small, right? Because otherwise this 1000 times W3 squared and 1000 times W4 square terms are going to be really, really big. So when you minimize this function, you're going to end up with W3 close to 0 and W4 close to 0. So we're effectively nearly canceling out the effects of the features execute and extra power of 4 and getting rid of these two terms over here. And if we do that, then we end up with a fit to the data that's much closer to the quadratic function, including maybe just tiny contributions from the features x cubed and extra 4. And this is good because it's a much better fit to the data compared to if all the parameters could be large and you end up with this weekly quadratic function more generally, here's the idea behind regularization. \n",
    "\n",
    "\n",
    "The idea is that if there are smaller values for the parameters, then that's a bit like having a simpler model. Maybe one with fewer features, which is therefore less prone to overfitting. On the last slide we penalize or we say we regularized only W3 and W4. But more generally, the way that regularization tends to be implemented is if you have a lot of features, say a 100 features, you may not know which are the most important features and which ones to penalize. \n",
    "\n",
    "So the way regularization is typically implemented is to penalize all of the features or more precisely, you penalize all the Wj parameters and it's possible to show that this will usually result in fitting a smoother simpler, less weekly function that's less prone to overfitting. \n",
    "\n",
    "So for this example, if you have data with 100 features for each house, it may be hard to pick an advance which features to include and which ones to exclude. So let's build a model that uses all 100 features. So you have these 100 parameters W1 through W100, as well as 100 and first parameter B. Because we don't know which of these parameters are going to be the important ones. Let's penalize all of them a bit and shrink all of them by adding this new term lambda times the sum from J equals 1 through n where n is 100. The number of features of wj squared. \n",
    "\n",
    "This value lambda here is the Greek alphabet lambda and it's also called a regularization parameter. So similar to picking a learning rate alpha, you now also have to choose a number for lambda. A couple of things I would like to point out by convention, instead of using lambda times the sum of Wj squared. We also divide lambda by 2m so that both the 1st and 2nd terms here are scaled by 1 over 2m. It turns out that by scaling both terms the same way it becomes a little bit easier to choose a good value for lambda. And in particular you find that even if your training set size growth, say you find more training examples. So m the training set size is now bigger. The same value of lambda that you've picked previously is now also more likely to continue to work if you have this extra scaling by 2m. \n",
    "\n",
    "Also by the way, by convention we're not going to penalize the parameter b for being large. In practice, it makes very little difference whether you do or not. And some machine learning engineers and actually some learning algorithm implementations will also include lambda over 2m times the b squared term. But this makes very little difference in practice and the more common convention which was used in this course is to regularize only the parameters w rather than the parameter b. \n",
    "\n",
    "<img src=\"images/cfr2.png\" style=\"width:700px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose value of **Lambda** very carefully.\n",
    "\n",
    "Very Large Lambda: Underfitting\n",
    "\n",
    "Very Small Lambda: Overfitting\n",
    "\n",
    "<img src=\"images/cfr3.png\" style=\"width:700px;height:400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized linear regression\n",
    "\n",
    "<img src=\"images/rlr1.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "<img src=\"images/rlr2.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "<img src=\"images/rlr3.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "<img src=\"images/rlr4.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "<img src=\"images/rlrq.png\" style=\"width:1000px;height:1100px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression\n",
    "\n",
    "<img src=\"images/rlgr1.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "<img src=\"images/rlgr2.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "<img src=\"images/rlgrq.png\" style=\"width:600px;height:600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
