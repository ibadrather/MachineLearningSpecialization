{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple features\n",
    "\n",
    "Instead of one we will have multiple features.\n",
    "\n",
    "<img src=\"images/mf1.png\" style=\"width:700px;height:340px;\">\n",
    "\n",
    "Now, the model will be:\n",
    "\n",
    "<img src=\"images/mf3.png\" style=\"width:700px;height:340px;\">\n",
    "\n",
    "\n",
    "<img src=\"images/mf4.png\" style=\"width:700px;height:340px;\">\n",
    "\n",
    "This is called multiple linear regression not multivaroate linear regression. That is something different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/v1.png\" style=\"width:1000px;height:400px;\">\n",
    "\n",
    "What is the computer doing behind the scenes to make this computation faster.\n",
    "\n",
    "<img src=\"images/v2.png\" style=\"width:1000px;height:400px;\">\n",
    "\n",
    "<img src=\"images/v3.png\" style=\"width:1000px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent for multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gd1.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/gd2.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "Alternative:\n",
    "\n",
    "<img src=\"images/gd3.png\" style=\"width:700px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling 1\n",
    "\n",
    "They will help make gradient descent faster.\n",
    "\n",
    "<img src=\"images/fs1.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/fs2.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/fs3.png\" style=\"width:700px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling 2\n",
    "\n",
    "How to do this scaling:\n",
    "\n",
    "<img src=\"images/fs4.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/fs4.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/fs4.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/fs5.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/fs6.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "<img src=\"images/fs7.png\" style=\"width:700px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking gradient descent for convergence\n",
    "\n",
    "<img src=\"images/gdc1.png\" style=\"width:700px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the learning rate\n",
    "\n",
    "If learning rate is too small, it will run very slowly and if it is too large, it may not even converge.\n",
    "\n",
    "<img src=\"images/gdc2.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "So if gradient descent isn't working, one thing I often do and I hope you find this tip useful too, one thing I'll often do is just set Alpha to be a very small number and see if that causes the cost to decrease on every iteration. If even with Alpha set to a very small number, J doesn't decrease on every single iteration, but instead sometimes increases, then that usually means there's a bug somewhere in the code. Note that setting Alpha to be really small is meant here as a debugging step and a very small value of Alpha is not going to be the most efficient choice for actually training your learning algorithm. One important trade-off is that if your learning rate is too small, then gradient descents can take a lot of iterations to converge. So when I am running gradient descent, I will usually try a range of values for the learning rate Alpha. I may start by trying a learning rate of 0.001 and I may also try learning rate as 10 times as large say 0.01 and 0.1 and so on. For each choice of Alpha, you might run gradient descent just for a handful of iterations and plot the cost function J as a function of the number of iterations and after trying a few different values, you might then pick the value of Alpha that seems to decrease the learning rate rapidly, but also consistently.\n",
    "\n",
    "<img src=\"images/gdc3.png\" style=\"width:700px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fe1.png\" style=\"width:700px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've just been fitting straight lines to our data. Let's take the ideas of multiple linear regression and feature engineering to come up with a new algorithm called polynomial regression, which will let you fit curves, non-linear functions, to your data. \n",
    "\n",
    "<img src=\"images/pr1.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "\n",
    "<img src=\"images/pr2.png\" style=\"width:700px;height:300px;\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deeply')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ba283c99aab3f9826c9057a6b77e5ed1375b1012fccd26237ed2bb4d681a306"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
