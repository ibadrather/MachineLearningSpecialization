{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook wil contain notes and good practices as advised by Andrew Ng during the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Supervised Learning**\n",
    "\n",
    "The two major types of **supervised learning** our regression and classification. \n",
    "\n",
    "1. In a **regression** application like predicting prices of houses, the learning algorithm has to predict numbers from infinitely many possible output numbers. \n",
    "\n",
    "2. Whereas in **classification** the learning algorithm has to make a prediction of a category, all of a small set of possible outputs. \n",
    "\n",
    "#### **Un-supervised Learning**\n",
    "\n",
    "We call it unsupervised because we're not trying to supervise the algorithm (There are no labels). To give some quote right answer for every input, instead, we asked the our room to figure out all by yourself what's interesting. Or what patterns or structures that might be in this data, with this particular data set. \n",
    "\n",
    "An unsupervised learning algorithm, might decide that the data can be assigned to two different groups or two different clusters. And so it might decide, that there's one cluster what group over here, and there's another cluster or group over here. This is a particular type of unsupervised learning, called a clustering algorithm. Because it places the unlabeled data, into different clusters and this turns out to be used in many applications. \n",
    "\n",
    "For example, clustering is used in google news, what google news does is every day it goes. And looks at hundreds of thousands of news articles on the internet, and groups related stories together.\n",
    "\n",
    "**Types of Unsupervised Learning:**\n",
    "1. Clustering\n",
    "2. Anomaly Detection\n",
    "3. Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**feature(x) -> [model(f)] -> prediction (ŷ)**\n",
    "\n",
    "Fwb(x) = wx + b also simply f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/arch.png\" style=\"width:600px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "f(x) = wx + b\n",
    "\n",
    "for different values of w and b the output y will vary.\n",
    "e.g:\n",
    "\n",
    "f(x) = b\n",
    "f(x) = 0.5*x\n",
    "f(x) = x + 1\n",
    "\n",
    "\n",
    "**error = (ŷ(i) - y(i))**\n",
    "\n",
    "Cost Function:  (Squared error cost function)\n",
    "\n",
    "<img src=\"images/cost_function.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "We want to find values of w and b that make the cost function small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost function Intuition\n",
    "\n",
    "here bias (b) = 0.0\n",
    "\n",
    "<img src=\"images/cost_function_visualised.png\" style=\"width:700px;height:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the cost function\n",
    "\n",
    "<img src=\"images/loss_function_2d.png\" style=\"width:700px;height:500px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contour Plot of the Cost Function**\n",
    "\n",
    "<img src=\"images/cost_function_contour.png\" style=\"width:1000px;height:600px;\">\n",
    "\n",
    "<img src=\"images/cost_function_contour_2.png\" style=\"width:800px;height:600px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/grad_desc_1.png\" style=\"width:800px;height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Algo\n",
    "\n",
    "<img src=\"images/grad_desc_algo.png\" style=\"width:800px;height:400px;\">\n",
    "\n",
    "<img src=\"images/grad_desc_intuition.png\" style=\"width:800px;height:400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lr1.png\" style=\"width:800px;height:400px;\">\n",
    "<img src=\"images/lr2.png\" style=\"width:800px;height:400px;\">\n",
    "<img src=\"images/lr3.png\" style=\"width:800px;height:400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gdlr1.png\" style=\"width:800px;height:400px;\">\n",
    "\n",
    "Sqaured error cost doesn't have a local minimum. It has one global minimum. (It is a convex function)\n",
    "\n",
    "<img src=\"images/gdlr2.png\" style=\"width:800px;height:400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent in action\n",
    "\n",
    "<img src=\"images/gdia.png\" style=\"width:800px;height:400px;\">\n",
    "<img src=\"images/bgd.png\" style=\"width:800px;height:400px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deeply')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ba283c99aab3f9826c9057a6b77e5ed1375b1012fccd26237ed2bb4d681a306"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
